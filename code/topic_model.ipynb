{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepocessing libraries\n",
    "import emoji\n",
    "import re\n",
    "from langdetect import detect\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def detect_lang(text):\n",
    "    \"\"\"\n",
    "    Detects the language of a story.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The story to be processed.\n",
    "    Returns\n",
    "    -------\n",
    "    lang : str\n",
    "        The language of the story.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "punc = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "spellcheck = SpellChecker()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a story by removing emojis, punctuations, stopwords, spellchecking and lemmatizing the words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The story to be preprocessed.\n",
    "    Returns\n",
    "    -------\n",
    "    processed_text : str\n",
    "        The preprocessed story.\n",
    "    \"\"\"\n",
    "\n",
    "    # regex to replace all consecutive occurences of punctuations with a single punctuation\n",
    "    pattern = r'([' + re.escape(''.join(punc)) + r'])\\1+'\n",
    "    text = re.sub(pattern, r'\\1', ''.join(text))\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    # remove stopwords, punctuations, emojis, correct and lemmatize the words\n",
    "    tokens = [spellcheck.correction(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if emoji.is_emoji(token) == False]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punc]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens if token]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "file = open(\"data/stories.csv\", \"r\")\n",
    "stories_array = []\n",
    "\n",
    "for line in file:\n",
    "    stories_array.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = pd.DataFrame(stories_array, columns=['story'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: `preprocess()` on a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an investigative journalist and did a research on the Sugar Mummy scam circus in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure. You will be scammed! Basically they have a pre-paid phone card with a generic profile photo. They all asure you they are not scammers. After giving them you name, age and civil status they will ask for 300-500 SGDs for a fee. They only accept bank transfer. Then when you have payed this they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON´T PAY ANYTHING\" They are all scammers/fraudsters/liers. Don´t fall for any sweet talk or promises, you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: -How can you tell a scammer is lying? Their lips move... My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All suger mummy agents ARE SCAMMERS!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigative journalist research sugar mummy scam circus singapore operate way one say contacted 6 agent locate site via whatsapp scammer might change name one thing 100 sure scammed basically pre paid phone card generic profile photo sure scammer giving name age civil status ask 300 500 sod fee accept bank transfer played ask 1400 1900 sad fee insurance promise bow monthly salary least 10500 sad conclusion pay anything scammer fraudster lie fall sweet talk promise fooled sugar mummy end rainbow matter say lot client recommend nothing say true old saying go tell scammer lying lip move investigation complete willing hand spy handling aware super mummy agent scammer\n"
     ]
    }
   ],
   "source": [
    "print(data['story'][3494])\n",
    "print(preprocess(data['story'][3494]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add language column\n",
    "data[\"language\"] = data[\"story\"].apply(detect_lang)\n",
    "\n",
    "# filter out non-english stories\n",
    "data = data[data[\"language\"] == \"en\"]\n",
    "\n",
    "# drop language column\n",
    "data = data.drop(columns=[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test processing on dataframe of 5 stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b2c8f58a284e2482edc6e468308a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing stories:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm # to show progress bar while iterating over the stories\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing stories\", colour='#ffaaff')\n",
    "data[\"story\"][0:5].progress_apply(preprocess).to_csv(\"data/test.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3289b5a18b2a46baa0ef738ad45dd7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing stories:   0%|          | 0/3489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = data[\"story\"].progress_apply(preprocess)\n",
    "preprocessed_data.to_csv(\"data/processed_stories.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "## LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
