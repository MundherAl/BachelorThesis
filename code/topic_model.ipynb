{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OpenAI\n",
    "import openai\n",
    "\n",
    "# read api key from file\n",
    "with open('resources/api-keys.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepocessing libraries\n",
    "import emoji\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\"\"\"\n",
    "Language detection\n",
    "\"\"\"\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "\"\"\"\n",
    "Auto-correction using GPT-3\n",
    "\"\"\"\n",
    "def gpt_autocorrect(text, t_max):\n",
    "    prompt_string = \"You will be receiving a text. Your task to correct any spelling mistakes that might be present in the text.  \" + text\n",
    "\n",
    "    r = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt_string,\n",
    "        temperature=0.2,\n",
    "        max_tokens=t_max,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=10,\n",
    "    )\n",
    "\n",
    "    return r['choices'][0]['text']\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # set of stopwords\n",
    "punc = set(string.punctuation) # set of all special characters\n",
    "lemma = WordNetLemmatizer() # lemmatizer\n",
    "tokenizer = WordPunctTokenizer() # tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing function\n",
    "\"\"\"\n",
    "def preprocess(text, max_tokens):\n",
    "    print(\"Correcting spelling mistakes...\")\n",
    "    text = gpt_autocorrect(text, max_tokens)\n",
    "    print(\"Preprocessing...\")\n",
    "    # regex to replace all consecutive occurences of punctuations with a single punctuation\n",
    "    pattern = r'([' + re.escape(''.join(punc)) + r'])\\1+'\n",
    "    text = re.sub(pattern, r'\\1', ''.join(text))\n",
    "\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    # keep token if it is not an emoji\n",
    "    tokens = [token for token in tokens if emoji.is_emoji(token) == False]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punc]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    print(\"Story processed!\")\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test for `gpt_autocorrect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am pretty bad at spelling and grammar, I feel so awful about it.\n",
      "\n",
      "\n",
      "I am pretty bad at spelling and grammar, I feel so awful about it.\n"
     ]
    }
   ],
   "source": [
    "correction = gpt_autocorrect(\"i am perrty bad at speling and grammer, i fel so awfel abt it\", 20)\n",
    "print(correction)\n",
    "\n",
    "correction = gpt_autocorrect(\"I am pretty bad at spelling and grammar, I feel so awful about it.\", 20)\n",
    "print(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "file = open(\"data/stories.csv\", \"r\")\n",
    "stories_array = []\n",
    "\n",
    "for line in file:\n",
    "    stories_array.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = pd.DataFrame(stories_array, columns=['story'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: `gpt_autocorrect()` and `clean()` on a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an investigative journalist and did a research on the Sugar Mummy scam circus in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure. You will be scammed! Basically they have a pre-paid phone card with a generic profile photo. They all asure you they are not scammers. After giving them you name, age and civil status they will ask for 300-500 SGDs for a fee. They only accept bank transfer. Then when you have payed this they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON´T PAY ANYTHING\" They are all scammers/fraudsters/liers. Don´t fall for any sweet talk or promises, you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: -How can you tell a scammer is lying? Their lips move... My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All suger mummy agents ARE SCAMMERS!\n",
      "\n",
      "\n",
      "I am an investigative journalist and did a research on the Sugar Mummy scam circuit in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure: you will be scammed! Basically, they have a pre-paid phone card with a generic profile photo. They all assure you they are not scammers. After giving them your name, age, and civil status, they will ask for 300-500 SGD for a fee. They only accept bank transfer. Then, when you have paid this, they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON'T PAY ANYTHING\"! They are all scammers/fraudsters/liars. Don't fall for any sweet talk or promises; you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: \"How can you tell a scammer is lying? Their lips move...\" My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All sugar mummy agents ARE SCAMMERS!\n",
      "\n",
      "\n",
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "investigative journalist research sugar mummy scam circuit singapore operate way one say contacted 6 agent locanto site via whatsapp scammer might change name one thing 100 sure scammed basically pre paid phone card generic profile photo assure scammer giving name age civil status ask 300 500 sgd fee accept bank transfer paid ask 1400 1900 sgd fee insurance promise bmw monthly salary least 10500 sgd conclusion pay anything \"! scammer fraudsters liar fall sweet talk promise fooled sugar mummy end rainbow matter say lot client recommend nothing say true old saying go tell scammer lying lip move .\" investigation complete willing hand spf handling aware sugar mummy agent scammer\n"
     ]
    }
   ],
   "source": [
    "print(data['story'][3494])\n",
    "print(gpt_autocorrect(data['story'][3494], 500))\n",
    "print(\"\\n\")\n",
    "print(preprocess(data['story'][3494], 500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: add regex to `preprocess()` to catch these cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add language column\n",
    "data[\"language\"] = data[\"story\"].apply(detect_lang)\n",
    "\n",
    "# filter out non-english stories\n",
    "data = data[data[\"language\"] == \"en\"]\n",
    "\n",
    "# drop language column\n",
    "data = data.drop(columns=[\"language\"])\n",
    "\n",
    "# get story with highest number of tokens (for GPT-3)\n",
    "max_tokens = int(data[\"story\"].apply(lambda story: len(word_tokenize(story))).max()) + 50 # 50 for margin of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "0    accepted friend request facebook common friend...\n",
      "1    good morning aaron received application notice...\n",
      "Name: story, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# cleaning two stories\n",
    "\n",
    "two_clean_stories = data['story'][0:2].apply(lambda story: preprocess(story, max_tokens))\n",
    "print(two_clean_stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "Story processed!\n",
      "Correcting spelling mistakes...\n"
     ]
    }
   ],
   "source": [
    "# clean all stories\n",
    "clean_stories = data['story'].apply(lambda story: preprocess(story, max_tokens))\n",
    "clean_stories.to_csv(\"data/clean_stories.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
