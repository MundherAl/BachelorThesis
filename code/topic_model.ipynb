{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OpenAI\n",
    "import openai\n",
    "\n",
    "# read api key from file\n",
    "with open('resources/api-keys.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepocessing libraries\n",
    "\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\"\"\"\n",
    "Language detection\n",
    "\"\"\"\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "\"\"\"\n",
    "Auto-correction using GPT-3\n",
    "\"\"\"\n",
    "def gpt_autocorrect(text, t_max):\n",
    "    prompt_string = \"You will be receiving a story. Your task to correct any spelling mistakes that might be present in the story. Here is the story: \" + text\n",
    "\n",
    "    r = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt_string,\n",
    "        temperature=0.3,\n",
    "        max_tokens=t_max,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=10,\n",
    "    )\n",
    "\n",
    "    return r['choices'][0]['text']\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # set of stopwords\n",
    "punc = set(string.punctuation) # set of all special characters\n",
    "lemma = WordNetLemmatizer() # lemmatizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing function\n",
    "\"\"\"\n",
    "def clean(text, max_tokens):\n",
    "    print(\"Correcting spelling mistakes...\")\n",
    "    text = gpt_autocorrect(text, max_tokens)\n",
    "    print(\"Preprocessing...\")\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punc]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test for `gpt_autocorrect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am pretty bad at spelling and grammar, I feel so awful about it.\n"
     ]
    }
   ],
   "source": [
    "correction = gpt_autocorrect(\"i am perrty bad at speling and grammer, i fel so awfel abt it\", 20)\n",
    "print(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "file = open(\"data/stories.csv\", \"r\")\n",
    "stories_array = []\n",
    "\n",
    "for line in file:\n",
    "    stories_array.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = pd.DataFrame(stories_array, columns=['story'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: `gpt_autocorrect()` and `clean()` on a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an investigative journalist and did a research on the Sugar Mummy scam circus in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure. You will be scammed! Basically they have a pre-paid phone card with a generic profile photo. They all asure you they are not scammers. After giving them you name, age and civil status they will ask for 300-500 SGDs for a fee. They only accept bank transfer. Then when you have payed this they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON´T PAY ANYTHING\" They are all scammers/fraudsters/liers. Don´t fall for any sweet talk or promises, you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: -How can you tell a scammer is lying? Their lips move... My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All suger mummy agents ARE SCAMMERS!\n",
      "\n",
      "\n",
      "I am an investigative journalist and did a research on the Sugar Mummy scam circuit in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure: you will be scammed! Basically, they have a pre-paid phone card with a generic profile photo. They all assure you they are not scammers. After giving them your name, age, and civil status, they will ask for 300-500 SGDs for a fee. They only accept bank transfer. Then, when you have paid this, they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON'T PAY ANYTHING!\" They are all scammers/fraudsters/liars. Don't fall for any sweet talk or promises; you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: \"How can you tell a scammer is lying? Their lips move...\" My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All sugar mummy agents ARE SCAMMERS!\n",
      "\n",
      "\n",
      "Correcting spelling mistakes...\n",
      "Preprocessing...\n",
      "investigative journalist research sugar mummy scam circuit singapore operate way one say contacted 6 agent locanto site via whatsapp scammer might change name one thing 100 sure scammed basically pre-paid phone card generic profile photo assure scammer giving name age civil status ask 300-500 sgds fee accept bank transfer paid ask 1400-1900 sgd fee insurance promise bmw monthly salary least 10500 sgd conclusion `` n't pay anything '' scammers/fraudsters/liars n't fall sweet talk promise fooled sugar mummy end rainbow matter say lot client recommend nothing say true old saying go `` tell scammer lying lip move ... '' investigation complete willing hand spf handling aware sugar mummy agent scammer\n"
     ]
    }
   ],
   "source": [
    "print(data['story'][3494])\n",
    "print(gpt_autocorrect(data['story'][3494], 500))\n",
    "print(\"\\n\")\n",
    "print(clean(data['story'][3494], 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [story, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add language column\n",
    "data[\"language\"] = data[\"story\"].apply(detect_lang)\n",
    "\n",
    "# filter out non-english stories\n",
    "data = data[data[\"language\"] == \"en\"]\n",
    "\n",
    "# clean the stories\n",
    "data[\"story\"] = data[\"story\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get story with highest number of tokens (for GPT-3)\n",
    "max_tokens = data[\"story\"].apply(lambda story: len(word_tokenize(story))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571115"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = data[\"story\"].apply(lambda story: len(word_tokenize(story))).sum()\n",
    "sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
