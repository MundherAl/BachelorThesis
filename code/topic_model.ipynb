{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepocessing libraries\n",
    "import emoji\n",
    "import re\n",
    "from langdetect import detect\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import itertools\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    \"\"\"\n",
    "    Detects the language of a story.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The story to be processed.\n",
    "    Returns\n",
    "    -------\n",
    "    lang : str\n",
    "        The language of the story.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "punc = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "spellcheck = SpellChecker()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a story by removing emojis, punctuations, stopwords, spellchecking and lemmatizing the words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The story to be preprocessed.\n",
    "    Returns\n",
    "    -------\n",
    "    processed_text : str\n",
    "        The preprocessed story.\n",
    "    \"\"\"\n",
    "\n",
    "    # regex to replace all consecutive occurences of punctuations with a single punctuation\n",
    "    pattern = r'([' + re.escape(''.join(punc)) + r'])\\1+'\n",
    "    text = re.sub(pattern, r'\\1', ''.join(text))\n",
    "\n",
    "    # regex to remove all numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    # remove stopwords, punctuations, emojis, correct and lemmatize the words\n",
    "    tokens = [spellcheck.correction(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if emoji.is_emoji(token) == False]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punc]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens if token]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def preprocess_without_lemmatizing(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a story by removing emojis, punctuations, stopwords, spellchecking the words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The story to be preprocessed.\n",
    "    Returns\n",
    "    -------\n",
    "    processed_text : str\n",
    "        The preprocessed story.\n",
    "    \"\"\"\n",
    "\n",
    "    # regex to replace all consecutive occurences of punctuations with a single punctuation\n",
    "    pattern = r'([' + re.escape(''.join(punc)) + r'])\\1+'\n",
    "    text = re.sub(pattern, r'\\1', ''.join(text))\n",
    "\n",
    "    # regex to remove all numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # tokenize the text\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    # remove stopwords, punctuations, emojis, correct and lemmatize the words\n",
    "    tokens = [spellcheck.correction(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if emoji.is_emoji(token) == False]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in punc]\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "file = open(\"data/stories.csv\", \"r\")\n",
    "stories_array = []\n",
    "\n",
    "for line in file:\n",
    "    stories_array.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "data = pd.DataFrame(stories_array, columns=['story'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: `preprocess()` on a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an investigative journalist and did a research on the Sugar Mummy scam circus in Singapore. They all operate the same way. No one is what they say they are. I contacted 6 of the agents on Locanto and other sites via WhatsApp and they were all scammers. They might change names but one thing is for 100% sure. You will be scammed! Basically they have a pre-paid phone card with a generic profile photo. They all asure you they are not scammers. After giving them you name, age and civil status they will ask for 300-500 SGDs for a fee. They only accept bank transfer. Then when you have payed this they ask for 1400-1900 SGD for further fees and insurance. They promise you a BMW and a monthly salary of at least 10500 SGD and so on. My conclusion is \"DON´T PAY ANYTHING\" They are all scammers/fraudsters/liers. Don´t fall for any sweet talk or promises, you will be fooled and no sugar mummy is at the end of the rainbow. No matter who they say they are or that they have lots of clients that recommend them, nothing they say is true. The old saying goes: -How can you tell a scammer is lying? Their lips move... My investigation is complete and I am willing to hand it over to the SPF for further handling. BE AWARE! All suger mummy agents ARE SCAMMERS!\n",
      "\n",
      "investigative journalist research sugar mummy scam circus singapore operate way one say contacted agent locate site via whatsapp scammer might change name one thing sure scammed basically pre paid phone card generic profile photo sure scammer giving name age civil status ask sod fee accept bank transfer played ask sad fee insurance promise bow monthly salary least sad conclusion pay anything scammer fraudster lie fall sweet talk promise fooled sugar mummy end rainbow matter say lot client recommend nothing say true old saying go tell scammer lying lip move investigation complete willing hand spy handling aware super mummy agent scammer\n"
     ]
    }
   ],
   "source": [
    "print(data['story'][3494])\n",
    "print(preprocess(data['story'][3494]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scammed fake website'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"I was scammed by a fake website\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add language column\n",
    "data[\"language\"] = data[\"story\"].apply(detect_lang)\n",
    "\n",
    "# filter out non-english stories\n",
    "data = data[data[\"language\"] == \"en\"]\n",
    "\n",
    "# drop language column\n",
    "data = data.drop(columns=[\"language\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test processing on dataframe of 5 stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Preprocessing stories: 100%|\u001b[38;2;255;170;255m██████████\u001b[0m| 5/5 [00:01<00:00,  4.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    accepted friend request facebook common friend...\n",
       "1    whatsapp message good morning baron ya receive...\n",
       "2    met alan bumble claimed project manager synerg...\n",
       "3    connected person named ano cab app june normal...\n",
       "4    person online name june lee initially contacte...\n",
       "Name: story, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Preprocessing stories\", colour='#ffaaff')\n",
    "data[\"story\"][0:5].progress_apply(preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing stories: 100%|\u001b[38;2;255;170;255m██████████\u001b[0m| 3489/3489 [08:01<00:00,  7.25it/s]  \n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = data[\"story\"].progress_apply(preprocess)\n",
    "preprocessed_data.to_csv(\"data/preprocessed_stories_lemmatized.csv\", index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data without lemmatizing and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing stories: 100%|\u001b[38;2;255;170;255m██████████\u001b[0m| 3489/3489 [08:10<00:00,  7.12it/s]  \n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = data[\"story\"].progress_apply(preprocess_without_lemmatizing)\n",
    "preprocessed_data.to_csv(\"data/preprocessed_stories.csv\", index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "## LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_perplexities(data, no_topics, min_df):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Not used in the project.\n",
    "\n",
    "    Calculates the perplexities of LDA models with different number of topics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.Series\n",
    "        The preprocessed stories.\n",
    "    no_topics : int\n",
    "        The maximum number of topics to be considered.\n",
    "    min_df : int\n",
    "        The minimum number of documents a word should appear in to be\n",
    "        considered as a feature.\n",
    "    Returns\n",
    "    -------\n",
    "    perplexities : list\n",
    "        The list of perplexities of LDA models with different nu\n",
    "    \"\"\"\n",
    "\n",
    "    progress_bar = tqdm(total=no_topics, desc='Calculating Perplexities', unit='model') # to show progress bar while iterating over the number of topics\n",
    "\n",
    "    perplexities = []\n",
    "    vectorized_data = CountVectorizer(min_df=min_df).fit_transform(data)\n",
    "\n",
    "    for i in range(2, no_topics+1):\n",
    "        lda = LatentDirichletAllocation(n_components=i, random_state=0)\n",
    "        lda.fit(vectorized_data)\n",
    "        perplexities.append(lda.perplexity(vectorized_data))\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return perplexities\n",
    "\n",
    "# function to display the topics of a topic model\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    # TODO: implement this function\n",
    "    pass\n",
    "            \n",
    "\n",
    "def get_coherence_score(model, text, dictionary, coherence):\n",
    "    \"\"\"\n",
    "    Calculates the coherence score of a topic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : gensim.models.ldamodel.LdaModel\n",
    "        The LDA model.\n",
    "    corpus : list\n",
    "        The list of preprocesses stories.\n",
    "    dictionary : gensim.corpora.dictionary.Dictionary\n",
    "        The dictionary of the corpus.\n",
    "    Returns\n",
    "    -------\n",
    "    coherence_score : float\n",
    "        The coherence score of the topic model.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the word2vec score of the topic model\n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=text, dictionary=dictionary, coherence=coherence)\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_cross_validation_data(data, n_topics_range, min_dfs, max_dfs, alphas, coherence):\n",
    "    \"\"\"\n",
    "    Performs cross validation to find the best hyperparameters for the topic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.Series\n",
    "        The preprocessed stories.\n",
    "    n_topics_range : list\n",
    "        The range of number of topics to be considered.\n",
    "    min_dfs : list\n",
    "        The range of minimum document frequencies to be considered.\n",
    "    max_dfs : list\n",
    "        The range of maximum document frequencies to be considered.\n",
    "    alphas : list\n",
    "        The range of alpha values to be considered.\n",
    "    Returns\n",
    "    -------\n",
    "    cross_validation_data : pandas.DataFrame\n",
    "        The cross validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_validation_data = pd.DataFrame(columns=['no_topics', 'min_df', 'max_df', 'alpha', coherence])\n",
    "    total_iterations = len(n_topics_range) * len(min_dfs) * len(max_dfs) * len(alphas)\n",
    "    progress_bar = tqdm(total=total_iterations, desc='Cross Validation', unit='model') # to show progress bar while iterating over the number of topics\n",
    "\n",
    "    # Grid search to find the best hyperparameters\n",
    "    for no_topics in n_topics_range:\n",
    "        for min_df in min_dfs:\n",
    "            for max_df in max_dfs:\n",
    "                for alpha in alphas:\n",
    "                    min_df_abs = min_df*data.size\n",
    "\n",
    "                    tokens_list = data[\"story\"].str.split().to_list()\n",
    "                    dictionary = Dictionary(tokens_list)\n",
    "                    dictionary.filter_extremes(no_below=min_df_abs, no_above=max_df)\n",
    "                    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "                    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=no_topics, alpha=alpha, random_state=0)\n",
    "                    coherence_score = get_coherence_score(lda, tokens_list, dictionary, coherence=coherence)\n",
    "\n",
    "                    cross_validation_data.loc[len(cross_validation_data)] = [no_topics, min_df, max_df, alpha, coherence_score]\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "                    \n",
    "    progress_bar.close()\n",
    "    return cross_validation_data\n",
    "\n",
    "def get_cross_validation_data_optimized(data, n_topics_range, min_dfs, max_dfs, alphas, coherence):\n",
    "    cross_validation_data = pd.DataFrame(columns=['no_topics', 'min_df', 'max_df', 'alpha', coherence])\n",
    "    total_iterations = len(n_topics_range) * len(min_dfs) * len(max_dfs) * len(alphas)\n",
    "    progress_bar = tqdm(total=total_iterations, desc='Cross Validation', unit='model')\n",
    "\n",
    "    parameter_combinations = itertools.product(n_topics_range, min_dfs, max_dfs, alphas)\n",
    "\n",
    "    for parameters in parameter_combinations:\n",
    "        no_topics, min_df, max_df, alpha = parameters\n",
    "        min_df_abs = min_df * data.size\n",
    "\n",
    "        tokens_list = data[\"story\"].str.split().to_list()\n",
    "        dictionary = Dictionary(tokens_list)\n",
    "        dictionary.filter_extremes(no_below=min_df_abs, no_above=max_df)\n",
    "        corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "        lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=no_topics, alpha=alpha, random_state=0)\n",
    "        coherence_score = get_coherence_score(lda, tokens_list, dictionary, coherence=coherence)\n",
    "\n",
    "        cross_validation_data.loc[len(cross_validation_data)] = [no_topics, min_df, max_df, alpha, coherence_score]\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return cross_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005576748461179599\n"
     ]
    }
   ],
   "source": [
    "# Coherence score test\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "data_lemmatized = pd.read_csv(\"data/preprocessed_stories_lemmatized.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "# tokenize the data for corpus and dictionary\n",
    "tokens_list = data[\"story\"].str.split().to_list()\n",
    "\n",
    "# create a dictionary of the data and filter out the extremes\n",
    "dictionary = Dictionary(tokens_list)\n",
    "dictionary.filter_extremes(no_below=0.01*data.size, no_above=0.99)\n",
    "\n",
    "# create a corpus of the data\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens_list]\n",
    "\n",
    "# get the coherence score of the topic model\n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=0, alpha=0.1)\n",
    "coherence_score = get_coherence_score(lda, tokens_list, dictionary)\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6563/3978630605.py:96: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm_notebook(total=total_iterations, desc='Cross Validation', unit='model') # to show progress bar while iterating over the number of topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9941ab4c57f841ef8aac3deb3f681a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross Validation:   0%|          | 0/16 [00:00<?, ?model/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_topics</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_df</th>\n",
       "      <th>alpha</th>\n",
       "      <th>coherence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_topics  min_df  max_df  alpha  coherence_score\n",
       "0        10.0    0.01    0.99   0.01         0.006149\n",
       "1        10.0    0.01    0.99   0.02         0.006149\n",
       "2        10.0    0.01    0.98   0.01         0.006149\n",
       "3        10.0    0.01    0.98   0.02         0.006149\n",
       "4        10.0    0.05    0.99   0.01         0.005869\n",
       "5        10.0    0.05    0.99   0.02         0.005869\n",
       "6        10.0    0.05    0.98   0.01         0.005869\n",
       "7        10.0    0.05    0.98   0.02         0.005869\n",
       "8        20.0    0.01    0.99   0.01        -0.000907\n",
       "9        20.0    0.01    0.99   0.02        -0.000381\n",
       "10       20.0    0.01    0.98   0.01        -0.000907\n",
       "11       20.0    0.01    0.98   0.02        -0.000381\n",
       "12       20.0    0.05    0.99   0.01         0.005038\n",
       "13       20.0    0.05    0.99   0.02         0.005279\n",
       "14       20.0    0.05    0.98   0.01         0.005038\n",
       "15       20.0    0.05    0.98   0.02         0.005279"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: get the cross validation data for two sets of hyperparameters\n",
    "cross_validation_data = get_cross_validation_data(data, [10, 20], [0.01, 0.05], [0.99, 0.98], [0.01, 0.02])\n",
    "cross_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_cross_validation_data() missing 1 required positional argument: 'coherence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m alphas \u001b[39m=\u001b[39m [i\u001b[39m*\u001b[39m\u001b[39m0.05\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m21\u001b[39m)]\n\u001b[1;32m      7\u001b[0m no_topics_range \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m21\u001b[39m)]\n\u001b[0;32m----> 9\u001b[0m cross_validation_data \u001b[39m=\u001b[39m get_cross_validation_data(data, no_topics_range, min_dfs, max_dfs, alphas)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cross_validation_data() missing 1 required positional argument: 'coherence'"
     ]
    }
   ],
   "source": [
    "# Cross validation step for non-lemmatized data\n",
    "data = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "min_dfs = [i*0.01 for i in range(1, 11)]\n",
    "max_dfs = [i*0.01 for i in range(99, 89, -1)]\n",
    "alphas = [i*0.05 for i in range(1, 21)]\n",
    "no_topics_range = [i for i in range(2, 21)]\n",
    "\n",
    "cross_validation_data = get_cross_validation_data(data, no_topics_range, min_dfs, max_dfs, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6563/3860172838.py:96: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm_notebook(total=total_iterations, desc='Cross Validation', unit='model') # to show progress bar while iterating over the number of topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b504b488c7d43c7ac4666db8ed1eb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross Validation:   0%|          | 0/38000 [00:00<?, ?model/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_data_lemmatized = get_cross_validation_data(data_lemmatized, no_topics_range, min_dfs, max_dfs, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross Validation: 100%|██████████| 16/16 [00:35<00:00,  2.24s/model]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "min_dfs = [0.01, 0.02]\n",
    "max_dfs = [0.99, 0.98]\n",
    "alphas = [0.01, 0.02]\n",
    "no_topics_range = [10, 20]\n",
    "\n",
    "cross_validation_data = get_cross_validation_data_optimized(data, no_topics_range, min_dfs, max_dfs, alphas, coherence='c_v')\n",
    "\n",
    "cross_validation_data.to_csv(\"data/cross_validation_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross Validation:  95%|█████████▌| 36170/38000 [19:58:34<1:18:07,  2.56s/model] "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "min_dfs = [i*0.01 for i in range(1, 11)]\n",
    "max_dfs = [i*0.01 for i in range(99, 89, -1)]\n",
    "alphas = [i*0.05 for i in range(1, 21)]\n",
    "no_topics_range = [i for i in range(2, 21)]\n",
    "\n",
    "cross_validation_data_cv = get_cross_validation_data(data, no_topics_range, min_dfs, max_dfs, alphas, coherence='c_v')\n",
    "cross_validation_data_cv.to_csv(\"data/cross_validation_data_cv.csv\", index=False)\n",
    "\n",
    "data_lemmatized = pd.read_csv(\"data/preprocessed_stories_lemmatized.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "cross_validation_data_lemmatized_cv = get_cross_validation_data(data_lemmatized, no_topics_range, min_dfs, max_dfs, alphas, coherence='c_v')\n",
    "cross_validation_data_lemmatized_cv.to_csv(\"data/cross_validation_data_lemmatized_cv.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to vectorize the text data to feed it into the LDA model. We use the `CountVectorizer()`. This has a parameter `min_df` which stands for minimum document frequency. For example, if we set it to 0.01, it will ignore words that appear in less than 1% of the documents.\n",
    "\n",
    "There are also words that appear too often in the documents. `CountVectorizer()` has a parameter `max_df` to ignore the words that appear too freuqently. For example, if we set it to 0.95, it will ignore words that appear in more than 95% of the documents.\n",
    "\n",
    "We also need to determine the optimal number of topics. For that, we compute the coherence value for different number of topics and choose the one with the highest coherence value.\n",
    "\n",
    "These three parameters will be calculated and the best value will be chosen using cross validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the optimal number of topics, we train LDA models with different number of topics and compute their perplexities. We then take the number of topics that gives the lowest perplexity.\n",
    "\n",
    "For LDA, we set `random_state=100` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexities:   0%|          | 0/25 [00:00<?, ?model/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:57<00:07,  7.38s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:52<00:07,  7.17s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:46<00:06,  6.95s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:44<00:06,  6.87s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:40<00:06,  6.69s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:33<00:06,  6.39s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:33<00:06,  6.40s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:23<00:05,  5.99s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:22<00:05,  5.93s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:23<00:05,  5.98s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:17<00:05,  5.71s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:11<00:05,  5.49s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:05<00:05,  5.22s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:02<00:05,  5.11s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [01:56<00:04,  4.86s/model]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_stories_lemmatized = pd.read_csv(\"data/preprocessed_stories_lemmatized.csv\", header=None, names=[\"story\"])\n",
    "# list of 0.01 to 0.15 with step size of 0.01\n",
    "min_df_list = [i/100 for i in range(1, 16)]\n",
    "\n",
    "sets_of_perplexities_lemmatized = [calculate_perplexities(preprocessed_stories_lemmatized[\"story\"], 25, i) for i in min_df_list]\n",
    "\n",
    "pd.DataFrame(sets_of_perplexities_lemmatized).to_csv(\"data/perplexities_set_lemmatized.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:50<00:07,  7.09s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:48<00:07,  7.03s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:42<00:06,  6.78s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:38<00:06,  6.62s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:34<00:06,  6.45s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:30<00:06,  6.25s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:26<00:06,  6.11s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:26<00:06,  6.11s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:17<00:05,  5.73s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:14<00:05,  5.60s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:11<00:05,  5.46s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:05<00:05,  5.21s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [02:02<00:05,  5.12s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [01:55<00:04,  4.82s/model]\n",
      "Calculating Perplexities:  96%|█████████▌| 24/25 [01:51<00:04,  4.65s/model]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_stories = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "sets_of_perplexities = [calculate_perplexities(preprocessed_stories[\"story\"], 25, i) for i in min_df_list]\n",
    "\n",
    "pd.DataFrame(sets_of_perplexities).to_csv(\"data/perplexities_set.csv\", index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we realize that perplexity is perhaps not the best metric if we are trying to tune `min_df` and `max_df`. These parameters reduce the size of the vocabulary which automatically leads to lower perplexity, meaning that setting `min_df` and `max_df` to very low and high values respectively will give us the lowest perplexity.\n",
    "\n",
    "Instead, we can use coherence scores. Coherence is a way to measure how interpretable the topics are to humans. There are a couple of ways to compute coherence scores.\n",
    "\n",
    "The CV Coherence is not recommended by its own author.\n",
    "\n",
    "UMass Coherence Score calculates how often two words `w1` and `w2` appear in the same document. It then compares this to how often `w1` appears in the corpus.\n",
    "\n",
    "UCI Coherence Score is based on sliding windows and pointwise mutual information (PMI). It calculates the co-occurrence of words in a window (for example 10 words) and compares it to the probability of the words appearing together in the corpus.\n",
    "\n",
    "The Word2Vec Coherence Score measures the similarity between words intra-topic and inter-topic. The idea is to maximize the similarity between words in the same topic and minimize the similarity between words in different topics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
