{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepocessing libraries\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import itertools\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_perplexities(data, no_topics, min_df):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Not used in the project.\n",
    "\n",
    "    Calculates the perplexities of LDA models with different number of topics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.Series\n",
    "        The preprocessed stories.\n",
    "    no_topics : int\n",
    "        The maximum number of topics to be considered.\n",
    "    min_df : int\n",
    "        The minimum number of documents a word should appear in to be\n",
    "        considered as a feature.\n",
    "    Returns\n",
    "    -------\n",
    "    perplexities : list\n",
    "        The list of perplexities of LDA models with different nu\n",
    "    \"\"\"\n",
    "\n",
    "    progress_bar = tqdm(total=no_topics, desc='Calculating Perplexities', unit='model') # to show progress bar while iterating over the number of topics\n",
    "\n",
    "    perplexities = []\n",
    "    vectorized_data = CountVectorizer(min_df=min_df).fit_transform(data)\n",
    "\n",
    "    for i in range(2, no_topics+1):\n",
    "        lda = LatentDirichletAllocation(n_components=i, random_state=0)\n",
    "        lda.fit(vectorized_data)\n",
    "        perplexities.append(lda.perplexity(vectorized_data))\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return perplexities\n",
    "\n",
    "# function to display the topics of a topic model\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    # TODO: implement this function\n",
    "    pass\n",
    "            \n",
    "\n",
    "def get_coherence_score(model, text, dictionary, coherence):\n",
    "    \"\"\"\n",
    "    Calculates the coherence score of a topic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : gensim.models.ldamodel.LdaModel\n",
    "        The LDA model.\n",
    "    corpus : list\n",
    "        The list of preprocesses stories.\n",
    "    dictionary : gensim.corpora.dictionary.Dictionary\n",
    "        The dictionary of the corpus.\n",
    "    Returns\n",
    "    -------\n",
    "    coherence_score : float\n",
    "        The coherence score of the topic model.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the word2vec score of the topic model\n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=text, dictionary=dictionary, coherence=coherence)\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_cross_validation_data(data, n_topics_range, min_dfs, max_dfs, alphas, coherence):\n",
    "    \"\"\"\n",
    "    Performs cross validation to find the best hyperparameters for the topic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.Series\n",
    "        The preprocessed stories.\n",
    "    n_topics_range : list\n",
    "        The range of number of topics to be considered.\n",
    "    min_dfs : list\n",
    "        The range of minimum document frequencies to be considered.\n",
    "    max_dfs : list\n",
    "        The range of maximum document frequencies to be considered.\n",
    "    alphas : list\n",
    "        The range of alpha values to be considered.\n",
    "    Returns\n",
    "    -------\n",
    "    cross_validation_data : pandas.DataFrame\n",
    "        The cross validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_validation_data = pd.DataFrame(columns=['no_topics', 'min_df', 'max_df', 'alpha', coherence])\n",
    "    total_iterations = len(n_topics_range) * len(min_dfs) * len(max_dfs) * len(alphas)\n",
    "    progress_bar = tqdm(total=total_iterations, desc='Cross Validation', unit='model') # to show progress bar while iterating over the number of topics\n",
    "\n",
    "    # Grid search to find the best hyperparameters\n",
    "    for no_topics in n_topics_range:\n",
    "        for min_df in min_dfs:\n",
    "            for max_df in max_dfs:\n",
    "                for alpha in alphas:\n",
    "                    min_df_abs = min_df*data.size\n",
    "\n",
    "                    tokens_list = data[\"story\"].str.split().to_list()\n",
    "                    dictionary = Dictionary(tokens_list)\n",
    "                    dictionary.filter_extremes(no_below=min_df_abs, no_above=max_df)\n",
    "                    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "                    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=no_topics, alpha=alpha, random_state=0)\n",
    "                    coherence_score = get_coherence_score(lda, tokens_list, dictionary, coherence=coherence)\n",
    "\n",
    "                    cross_validation_data.loc[len(cross_validation_data)] = [no_topics, min_df, max_df, alpha, coherence_score]\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "                    \n",
    "    progress_bar.close()\n",
    "    return cross_validation_data\n",
    "\n",
    "def get_cross_validation_data_optimized(data, n_topics_range, min_dfs, max_dfs, alphas, coherence):\n",
    "    cross_validation_data = pd.DataFrame(columns=['no_topics', 'min_df', 'max_df', 'alpha', coherence])\n",
    "    total_iterations = len(n_topics_range) * len(min_dfs) * len(max_dfs) * len(alphas)\n",
    "    progress_bar = tqdm(total=total_iterations, desc='Cross Validation', unit='model')\n",
    "\n",
    "    parameter_combinations = itertools.product(n_topics_range, min_dfs, max_dfs, alphas)\n",
    "\n",
    "    for parameters in parameter_combinations:\n",
    "        no_topics, min_df, max_df, alpha = parameters\n",
    "        min_df_abs = min_df * data.size\n",
    "\n",
    "        tokens_list = data[\"story\"].str.split().to_list()\n",
    "        dictionary = Dictionary(tokens_list)\n",
    "        dictionary.filter_extremes(no_below=min_df_abs, no_above=max_df)\n",
    "        corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "        lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=no_topics, alpha=alpha, random_state=0)\n",
    "        coherence_score = get_coherence_score(lda, tokens_list, dictionary, coherence=coherence)\n",
    "\n",
    "        cross_validation_data.loc[len(cross_validation_data)] = [no_topics, min_df, max_df, alpha, coherence_score]\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return cross_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "# test: get the cross validation data for two sets of hyperparameters\n",
    "cross_validation_data = get_cross_validation_data(data, [10, 20], [0.01, 0.05], [0.99, 0.98], [0.01, 0.02])\n",
    "cross_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/preprocessed_stories.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "min_dfs = [i*0.01 for i in range(1, 11)]\n",
    "max_dfs = [i*0.01 for i in range(99, 89, -1)]\n",
    "alphas = [i*0.05 for i in range(1, 21)]\n",
    "no_topics_range = [i for i in range(2, 21)]\n",
    "\n",
    "cross_validation_data_cv = get_cross_validation_data(data, no_topics_range, min_dfs, max_dfs, alphas, coherence='c_v')\n",
    "cross_validation_data_cv.to_csv(\"data/cross_validation_data_cv.csv\", index=False)\n",
    "\n",
    "data_lemmatized = pd.read_csv(\"data/preprocessed_stories_lemmatized.csv\", header=None, names=[\"story\"])\n",
    "\n",
    "cross_validation_data_lemmatized_cv = get_cross_validation_data(data_lemmatized, no_topics_range, min_dfs, max_dfs, alphas, coherence='c_v')\n",
    "cross_validation_data_lemmatized_cv.to_csv(\"data/cross_validation_data_lemmatized_cv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
